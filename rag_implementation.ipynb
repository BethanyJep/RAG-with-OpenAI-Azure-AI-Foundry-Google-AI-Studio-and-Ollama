{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c023cb8",
   "metadata": {},
   "source": [
    "# Employee Handbook RAG Implementation with Multiple AI Platforms\n",
    "\n",
    "This notebook demonstrates how to implement Retrieval-Augmented Generation (RAG) for an employee handbook using different AI platforms:\n",
    "- OpenAI GPT models\n",
    "- Azure AI Foundry (Azure OpenAI)\n",
    "- Google AI Studio (Gemini)\n",
    "- Ollama (Local models)\n",
    "\n",
    "We'll build a knowledge base from the employee handbook and show how each platform can be used to answer HR-related questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4c13fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (1.79.0)\n",
      "Requirement already satisfied: chromadb in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (1.0.15)\n",
      "Requirement already satisfied: sentence-transformers in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (5.0.0)\n",
      "Requirement already satisfied: google-generativeai in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (0.8.5)\n",
      "Requirement already satisfied: ollama in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (0.4.8)\n",
      "Requirement already satisfied: python-dotenv in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (1.1.0)\n",
      "Requirement already satisfied: tiktoken in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (0.9.0)\n",
      "Requirement already satisfied: PyPDF2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (3.0.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from openai) (2.10.6)\n",
      "Requirement already satisfied: sniffio in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from openai) (4.13.2)\n",
      "Requirement already satisfied: idna>=2.8 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n",
      "Requirement already satisfied: build>=1.0.3 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from chromadb) (1.2.2.post1)\n",
      "Requirement already satisfied: pybase64>=1.4.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from chromadb) (1.4.1)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.35.0)\n",
      "Requirement already satisfied: numpy>=1.22.5 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from chromadb) (2.2.6)\n",
      "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from chromadb) (3.25.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from chromadb) (1.22.0)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from chromadb) (1.34.1)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from chromadb) (1.34.1)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from chromadb) (1.34.1)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from chromadb) (0.21.2)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from chromadb) (1.72.0rc1)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from chromadb) (4.3.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from chromadb) (0.15.4)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from chromadb) (33.1.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from chromadb) (9.1.2)\n",
      "Requirement already satisfied: pyyaml>=6.0.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from chromadb) (6.0.2)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from chromadb) (5.1.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from chromadb) (3.10.18)\n",
      "Requirement already satisfied: rich>=10.11.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from chromadb) (14.0.0)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from chromadb) (4.24.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.7 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.32.3)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: monotonic>=1.5 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: python-dateutil>2.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests<3.0,>=2.7->posthog<6.0.0,>=2.4.0->chromadb) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests<3.0,>=2.7->posthog<6.0.0,>=2.4.0->chromadb) (2.4.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from sentence-transformers) (4.53.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from sentence-transformers) (2.7.1)\n",
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from sentence-transformers) (1.7.0)\n",
      "Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from sentence-transformers) (1.16.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from sentence-transformers) (0.33.2)\n",
      "Requirement already satisfied: Pillow in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from sentence-transformers) (11.2.1)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.5.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from google-generativeai) (0.6.15)\n",
      "Requirement already satisfied: google-api-core in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from google-generativeai) (2.24.2)\n",
      "Requirement already satisfied: google-api-python-client in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from google-generativeai) (2.170.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from google-generativeai) (2.40.1)\n",
      "Requirement already satisfied: protobuf in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from google-generativeai) (5.29.4)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from google-api-core->google-generativeai) (1.70.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from rsa<5,>=3.1.4->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
      "Requirement already satisfied: pyproject_hooks in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from jsonschema>=4.19.0->chromadb) (0.26.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from kubernetes>=28.1.0->chromadb) (3.3.1)\n",
      "Requirement already satisfied: durationpy>=0.7 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
      "Requirement already satisfied: coloredlogs in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
      "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from onnxruntime>=1.14.1->chromadb) (1.14.0)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.34.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.34.1)\n",
      "Requirement already satisfied: opentelemetry-proto==1.34.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.34.1)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.55b1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.55b1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from rich>=10.11.0->chromadb) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (80.9.0)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: click<8.2,>=8.0.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from typer>=0.9.0->chromadb) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
      "Requirement already satisfied: uvloop>=0.15.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "# Install required packages!\n",
    "!pip install openai chromadb sentence-transformers google-generativeai ollama python-dotenv tiktoken PyPDF2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bef277d",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ece4013",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "from dotenv import load_dotenv\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import tiktoken\n",
    "import PyPDF2\n",
    "\n",
    "# AI Platform imports\n",
    "import openai\n",
    "from openai import AzureOpenAI\n",
    "import google.generativeai as genai\n",
    "import ollama\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize embedding model for vector storage\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "print(\"✅ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481da810",
   "metadata": {},
   "source": [
    "## 2. Configuration and Environment Setup\n",
    "\n",
    "Create a `.env` file in your project directory with the following variables:\n",
    "```\n",
    "OPENAI_API_KEY=your_openai_api_key\n",
    "AZURE_OPENAI_API_KEY=your_azure_openai_api_key\n",
    "AZURE_OPENAI_ENDPOINT=your_azure_openai_api_endpoint\n",
    "AZURE_OPENAI_API_VERSION=your_azure_openai_api_version\n",
    "GOOGLE_API_KEY=your_google_api_key\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7adb1d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Configuration loaded\n"
     ]
    }
   ],
   "source": [
    "# Configuration for different AI platforms\n",
    "class RAGConfig:\n",
    "    def __init__(self):\n",
    "        # OpenAI Configuration\n",
    "        self.openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        \n",
    "        # Azure OpenAI Configuration\n",
    "        self.azure_api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "        self.azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "        self.azure_api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2025-01-01-preview\")\n",
    "        \n",
    "        # Google AI Configuration\n",
    "        self.google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "        \n",
    "        # Ollama Configuration (assumes local installation)\n",
    "        self.ollama_host = \"http://localhost:11434\"\n",
    "        \n",
    "        # Vector DB Configuration\n",
    "        self.collection_name = \"employee_handbook_documents\"\n",
    "        \n",
    "        # Employee Handbook PDF path\n",
    "        self.handbook_pdf_path = \"employee_handbook (2).pdf\"\n",
    "\n",
    "config = RAGConfig()\n",
    "print(\"✅ Configuration loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e6e2831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ OpenAI client initialized\n",
      "✅ Azure OpenAI client initialized\n",
      "✅ Google AI client initialized\n",
      "✅ Ollama client initialized\n"
     ]
    }
   ],
   "source": [
    "# Initialize clients\n",
    "def initialize_clients():\n",
    "    clients = {}\n",
    "    \n",
    "    # OpenAI Client\n",
    "    if config.openai_api_key:\n",
    "        openai.api_key = config.openai_api_key\n",
    "        openai.api_type = \"openai\"  # Explicitly set the API type to OpenAI\n",
    "        clients['openai'] = openai\n",
    "        print(\"✅ OpenAI client initialized\")\n",
    "    \n",
    "    # Azure OpenAI Client\n",
    "    if config.azure_api_key and config.azure_endpoint:\n",
    "        clients['azure'] = AzureOpenAI(\n",
    "            api_key=config.azure_api_key,\n",
    "            api_version=config.azure_api_version,\n",
    "            azure_endpoint=config.azure_endpoint\n",
    "        )\n",
    "        print(\"✅ Azure OpenAI client initialized\")\n",
    "    \n",
    "    # Google AI Client\n",
    "    if config.google_api_key:\n",
    "        genai.configure(api_key=config.google_api_key)\n",
    "        clients['google'] = genai\n",
    "        print(\"✅ Google AI client initialized\")\n",
    "    \n",
    "    # Ollama Client (check if running)\n",
    "    try:\n",
    "        ollama_response = ollama.list()\n",
    "        clients['ollama'] = ollama\n",
    "        print(\"✅ Ollama client initialized\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Ollama not available: {e}\")\n",
    "    \n",
    "    return clients\n",
    "\n",
    "clients = initialize_clients()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aef5841",
   "metadata": {},
   "source": [
    "## 3. Employee Handbook Processing and Vector Store Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c176749e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Extracted 1 sections from employee handbook\n",
      "\n",
      "Handbook sections found:\n",
      "1. This document contains information generated using a language model (Azure OpenAI). The\n"
     ]
    }
   ],
   "source": [
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"Extract text content from PDF file.\"\"\"\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            text = \"\"\n",
    "            \n",
    "            for page_num in range(len(pdf_reader.pages)):\n",
    "                page = pdf_reader.pages[page_num]\n",
    "                text += page.extract_text() + \"\\n\"\n",
    "            \n",
    "            return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading PDF: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def create_handbook_sections(text: str) -> List[Dict]:\n",
    "    \"\"\"Split handbook text into logical sections.\"\"\"\n",
    "    # Split by common section markers or use paragraph breaks\n",
    "    sections = []\n",
    "    \n",
    "    # Simple approach: split by double newlines and filter out short sections\n",
    "    raw_sections = text.split('\\n\\n')\n",
    "    \n",
    "    section_id = 1\n",
    "    for section in raw_sections:\n",
    "        section = section.strip()\n",
    "        \n",
    "        # Skip very short sections (likely headers or artifacts)\n",
    "        if len(section) < 100:\n",
    "            continue\n",
    "            \n",
    "        # Try to identify section titles (usually shorter lines at the start)\n",
    "        lines = section.split('\\n')\n",
    "        if lines:\n",
    "            title = lines[0].strip()\n",
    "            # If first line is very long, use a generic title\n",
    "            if len(title) > 100:\n",
    "                title = f\"Employee Handbook Section {section_id}\"\n",
    "            \n",
    "            sections.append({\n",
    "                \"id\": f\"handbook_section_{section_id}\",\n",
    "                \"title\": title,\n",
    "                \"content\": section\n",
    "            })\n",
    "            section_id += 1\n",
    "    \n",
    "    return sections\n",
    "\n",
    "# Load and process employee handbook\n",
    "handbook_text = extract_text_from_pdf(config.handbook_pdf_path)\n",
    "\n",
    "if handbook_text:\n",
    "    handbook_sections = create_handbook_sections(handbook_text)\n",
    "    print(f\"✅ Extracted {len(handbook_sections)} sections from employee handbook\")\n",
    "    \n",
    "    # Display first few section titles\n",
    "    print(\"\\nHandbook sections found:\")\n",
    "    for i, section in enumerate(handbook_sections[:5]):\n",
    "        print(f\"{i+1}. {section['title']}\")\n",
    "    if len(handbook_sections) > 5:\n",
    "        print(f\"... and {len(handbook_sections) - 5} more sections\")\n",
    "else:\n",
    "    # Fallback: Create sample employee handbook data if PDF reading fails\n",
    "    print(\"⚠️ Could not read PDF file. Using sample employee handbook data...\")\n",
    "    handbook_sections = [\n",
    "        {\n",
    "            \"id\": \"handbook_section_1\",\n",
    "            \"title\": \"Company Overview and Mission\",\n",
    "            \"content\": \"\"\"Our company is committed to fostering an inclusive, innovative workplace where every employee can thrive. Founded in 2010, we have grown from a small startup to a leading technology company with over 500 employees worldwide. Our mission is to create cutting-edge solutions that improve people's lives while maintaining the highest standards of ethics and integrity. We value collaboration, innovation, continuous learning, and work-life balance.\"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"handbook_section_2\",\n",
    "            \"title\": \"Employment Policies and Procedures\",\n",
    "            \"content\": \"\"\"All employees are expected to maintain professional conduct and adhere to company policies. Working hours are typically 9 AM to 5 PM, Monday through Friday, with flexibility for remote work arrangements. Employees are entitled to 20 days of paid vacation annually, plus standard holidays. Performance reviews are conducted annually with opportunities for career advancement and professional development.\"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"handbook_section_3\",\n",
    "            \"title\": \"Benefits and Compensation\",\n",
    "            \"content\": \"\"\"We offer comprehensive benefits including health insurance, dental and vision coverage, retirement savings plan with company matching, life insurance, and professional development allowances. Employees receive competitive salaries reviewed annually, performance-based bonuses, stock options for eligible positions, and reimbursement for job-related training and certifications.\"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"handbook_section_4\",\n",
    "            \"title\": \"Code of Conduct and Ethics\",\n",
    "            \"content\": \"\"\"All employees must maintain the highest standards of professional and ethical conduct. This includes respecting colleagues regardless of background, maintaining confidentiality of proprietary information, avoiding conflicts of interest, and reporting any violations of company policy. Discrimination, harassment, or retaliation of any kind will not be tolerated and will result in disciplinary action up to and including termination.\"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"handbook_section_5\",\n",
    "            \"title\": \"Remote Work and Technology Policies\",\n",
    "            \"content\": \"\"\"Employees may work remotely up to 3 days per week with manager approval. Remote workers must maintain secure internet connections, use company-provided VPN for accessing internal systems, and participate in all required meetings via video conference. Company equipment must be used responsibly and returned upon termination. Personal use of company technology should be minimal and appropriate.\"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"handbook_section_6\",\n",
    "            \"title\": \"Leave Policies and Time Off\",\n",
    "            \"content\": \"\"\"In addition to vacation time, employees are entitled to sick leave, personal days, bereavement leave, and parental leave. Sick leave accrues at 1 day per month worked. Parental leave includes 12 weeks paid leave for primary caregivers and 6 weeks for secondary caregivers. All leave requests must be submitted through the HR system with appropriate advance notice when possible.\"\"\"\n",
    "        }\n",
    "    ]\n",
    "    print(f\"✅ Using {len(handbook_sections)} sample employee handbook sections\")\n",
    "\n",
    "# Store the handbook sections for later use\n",
    "sample_documents = handbook_sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6059b516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processed 7 chunks from 1 handbook sections\n"
     ]
    }
   ],
   "source": [
    "def chunk_text(text: str, max_tokens: int = 500) -> List[str]:\n",
    "    \"\"\"Split text into smaller chunks for better retrieval.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    tokens = encoding.encode(text)\n",
    "    \n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), max_tokens):\n",
    "        chunk_tokens = tokens[i:i + max_tokens]\n",
    "        chunk_text = encoding.decode(chunk_tokens)\n",
    "        chunks.append(chunk_text)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Process handbook sections into chunks\n",
    "processed_chunks = []\n",
    "for doc in sample_documents:\n",
    "    chunks = chunk_text(doc[\"content\"])\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        processed_chunks.append({\n",
    "            \"id\": f\"{doc['id']}_chunk_{i}\",\n",
    "            \"doc_id\": doc[\"id\"],\n",
    "            \"title\": doc[\"title\"],\n",
    "            \"content\": chunk,\n",
    "            \"metadata\": {\"source\": doc[\"title\"], \"type\": \"employee_handbook\"}\n",
    "        })\n",
    "\n",
    "print(f\"✅ Processed {len(processed_chunks)} chunks from {len(sample_documents)} handbook sections\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5b62895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ChromaDB collection created\n"
     ]
    }
   ],
   "source": [
    "# Initialize ChromaDB vector store\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "# Create or get collection\n",
    "try:\n",
    "    collection = chroma_client.get_collection(name=config.collection_name)\n",
    "    chroma_client.delete_collection(name=config.collection_name)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "collection = chroma_client.create_collection(\n",
    "    name=config.collection_name,\n",
    "    metadata={\"description\": \"RAG document collection\"}\n",
    ")\n",
    "\n",
    "print(\"✅ ChromaDB collection created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df8eec72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Added 7 documents to vector store\n"
     ]
    }
   ],
   "source": [
    "# Generate embeddings and add to collection\n",
    "def add_documents_to_vectorstore(chunks: List[Dict]):\n",
    "    \"\"\"Add document chunks to the vector store.\"\"\"\n",
    "    documents = [chunk[\"content\"] for chunk in chunks]\n",
    "    metadatas = [{\"doc_id\": chunk[\"doc_id\"], \"title\": chunk[\"title\"]} for chunk in chunks]\n",
    "    ids = [chunk[\"id\"] for chunk in chunks]\n",
    "    \n",
    "    # Generate embeddings\n",
    "    embeddings = embedding_model.encode(documents).tolist()\n",
    "    \n",
    "    # Add to collection\n",
    "    collection.add(\n",
    "        documents=documents,\n",
    "        embeddings=embeddings,\n",
    "        metadatas=metadatas,\n",
    "        ids=ids\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ Added {len(chunks)} documents to vector store\")\n",
    "\n",
    "add_documents_to_vectorstore(processed_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb58754",
   "metadata": {},
   "source": [
    "## 4. RAG Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ea54ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ RAG utility functions defined\n"
     ]
    }
   ],
   "source": [
    "def retrieve_relevant_docs(query: str, n_results: int = 3) -> List[Dict]:\n",
    "    \"\"\"Retrieve relevant documents for a given query.\"\"\"\n",
    "    query_embedding = embedding_model.encode([query]).tolist()\n",
    "    \n",
    "    results = collection.query(\n",
    "        query_embeddings=query_embedding,\n",
    "        n_results=n_results\n",
    "    )\n",
    "    \n",
    "    relevant_docs = []\n",
    "    for i in range(len(results['documents'][0])):\n",
    "        relevant_docs.append({\n",
    "            \"content\": results['documents'][0][i],\n",
    "            \"metadata\": results['metadatas'][0][i],\n",
    "            \"distance\": results['distances'][0][i]\n",
    "        })\n",
    "    \n",
    "    return relevant_docs\n",
    "\n",
    "def create_rag_prompt(query: str, context_docs: List[Dict]) -> str:\n",
    "    \"\"\"Create a prompt with context for RAG.\"\"\"\n",
    "    context = \"\\n\\n\".join([f\"Source: {doc['metadata']['title']}\\nContent: {doc['content']}\" \n",
    "                          for doc in context_docs])\n",
    "    \n",
    "    prompt = f\"\"\"You are a helpful HR assistant that answers questions about company policies and procedures based on the employee handbook. Use the following context to answer the user's question. If the answer cannot be found in the context, say so clearly.\n",
    "\n",
    "Context from Employee Handbook:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "print(\"✅ RAG utility functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7bd03e",
   "metadata": {},
   "source": [
    "## 5. OpenAI RAG Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afd57711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI RAG Result:\n",
      "Answer: The company's mission is to provide the highest quality aircraft components to customers while maintaining a commitment to safety and excellence. They strive to continually improve their products and services, aiming to remain a leader in the aerospace industry for years to come.\n",
      "Sources: ['This document contains information generated using a language model (Azure OpenAI). The', 'This document contains information generated using a language model (Azure OpenAI). The', 'This document contains information generated using a language model (Azure OpenAI). The']\n"
     ]
    }
   ],
   "source": [
    "def openai_rag_query(query: str) -> Dict[str, Any]:\n",
    "    \"\"\"Perform RAG query using OpenAI.\"\"\"\n",
    "    # Retrieve relevant documents\n",
    "    relevant_docs = retrieve_relevant_docs(query)\n",
    "    \n",
    "    # Create prompt with context\n",
    "    prompt = create_rag_prompt(query, relevant_docs)\n",
    "    \n",
    "    # Generate response\n",
    "    response = clients['openai'].chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful HR assistant that answers questions about company policies based on the employee handbook.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.1,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"platform\": \"OpenAI\",\n",
    "        \"model\": \"gpt-3.5-turbo\",\n",
    "        \"answer\": response.choices[0].message.content,\n",
    "        \"sources\": [doc['metadata']['title'] for doc in relevant_docs],\n",
    "        \"retrieved_docs\": relevant_docs\n",
    "    }\n",
    "\n",
    "# Test OpenAI RAG\n",
    "test_query = \"What are the company's mission?\"\n",
    "openai_result = openai_rag_query(test_query)\n",
    "print(\"OpenAI RAG Result:\")\n",
    "print(f\"Answer: {openai_result.get('answer')}\")\n",
    "print(f\"Sources: {openai_result.get('sources')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e7591b",
   "metadata": {},
   "source": [
    "## 6. Azure OpenAI RAG Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c40c208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AzureOpenAI RAG Result:\n",
      "Answer: The context provided does not specify the exact metrics used for performance reviews at Contoso Electronics. It mentions that performance reviews include a discussion of the employee's performance over the past year, feedback on areas for improvement, and goals and objectives for the upcoming year. Employees also receive a written summary that includes a rating of their performance, feedback, and goals. However, the specific metrics or criteria used to evaluate performance are not detailed in the employee handbook.\n",
      "Sources: ['This document contains information generated using a language model (Azure OpenAI). The', 'This document contains information generated using a language model (Azure OpenAI). The', 'This document contains information generated using a language model (Azure OpenAI). The']\n"
     ]
    }
   ],
   "source": [
    "def azure_rag_query(query: str) -> Dict[str, Any]:\n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"), \n",
    "        api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n",
    "        api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "    )\n",
    "    # Retrieve relevant documents\n",
    "    relevant_docs = retrieve_relevant_docs(query)\n",
    "            \n",
    "    # Create prompt with context\n",
    "    prompt = create_rag_prompt(query, relevant_docs)\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "                model=\"gpt-4o\",  # Use your Azure deployment name\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful HR assistant that answers questions about company policies based on the employee handbook.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.1,\n",
    "                max_tokens=500\n",
    "            )\n",
    "    return {\n",
    "            \"platform\": \"Azure OpenAI\",\n",
    "            \"model\": \"gpt-4o\",\n",
    "            \"answer\": response.choices[0].message.content,\n",
    "            \"sources\": [doc['metadata']['title'] for doc in relevant_docs],\n",
    "            \"retrieved_docs\": relevant_docs\n",
    "        }\n",
    "\n",
    "query = \"What are the performance reviews metrics?\"\n",
    "AzureOpenAIResult = azure_rag_query(query)\n",
    "print(\"AzureOpenAI RAG Result:\")\n",
    "print(f\"Answer: {AzureOpenAIResult.get('answer')}\")\n",
    "print(f\"Sources: {AzureOpenAIResult.get('sources')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5720b6d8",
   "metadata": {},
   "source": [
    "## 7. Google AI Studio (Gemini) RAG Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "427d8356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google AI RAG Result:\n",
      "Answer: The Whistleblower Policy at Contoso Electronics is established to encourage employees to report any unethical or illegal activities they may witness.\n",
      "\n",
      "Here are the key details you need to know:\n",
      "\n",
      "*   **Applicability:** This policy applies to all Contoso Electronics employees, contractors, and other third parties.\n",
      "*   **Definition of a Whistleblower:** An individual who reports activities that are illegal, unethical, or otherwise not in accordance with company policy.\n",
      "*   **Reporting Procedures:** If you witness any activity you believe to be illegal, unethical, or not in accordance with company policy, you should report it immediately by:\n",
      "    1.  Contacting the Human Resources Department.\n",
      "    2.  Emailing the Compliance Officer at compliance@contoso.com.\n",
      "    3.  Calling the Compliance Hotline at 1-800-555-1212.\n",
      "*   **Information to Provide When Reporting:** Please include as much detail as possible, such as:\n",
      "    1.  The time and date of the incident.\n",
      "    2.  Who was involved.\n",
      "    3.  What happened.\n",
      "    4.  Any evidence you may have related to the incident.\n",
      "*   **Anonymous Reporting:** You may report anonymously by calling the Compliance Hotline at 1-800-555-1212.\n",
      "*   **Retaliation Prohibited:** Retaliation of any kind against a whistleblower is strictly prohibited. Any employee who retaliates will be subject to disciplinary action, up to and including termination.\n",
      "*   **Confidentiality:** The identity of the whistleblower will be kept confidential to the extent permitted by law.\n",
      "*   **Investigation:** All reported incidents will be investigated promptly and thoroughly.\n",
      "Sources: ['This document contains information generated using a language model (Azure OpenAI). The', 'This document contains information generated using a language model (Azure OpenAI). The', 'This document contains information generated using a language model (Azure OpenAI). The']\n"
     ]
    }
   ],
   "source": [
    "def google_rag_query(query: str) -> Dict[str, Any]:\n",
    "    \"\"\"Perform RAG query using Google AI Studio.\"\"\"\n",
    "    from google import genai\n",
    "    \n",
    "    # Initialize client\n",
    "    client = genai.Client(api_key=config.google_api_key)\n",
    "    \n",
    "    # Retrieve relevant documents\n",
    "    relevant_docs = retrieve_relevant_docs(query)\n",
    "    \n",
    "    # Create prompt with context\n",
    "    prompt = create_rag_prompt(query, relevant_docs)\n",
    "    \n",
    "    # Generate response\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash\", \n",
    "        contents=prompt\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"platform\": \"Google AI Studio\",\n",
    "        \"model\": \"gemini-2.5-flash\",\n",
    "        \"answer\": response.text,\n",
    "        \"sources\": [doc['metadata']['title'] for doc in relevant_docs],\n",
    "        \"retrieved_docs\": relevant_docs\n",
    "    }\n",
    "\n",
    "# Test Google AI RAG\n",
    "test_query = \"What are the whistle blower details i need to know?\"\n",
    "google_result = google_rag_query(test_query)\n",
    "print(\"Google AI RAG Result:\")\n",
    "print(f\"Answer: {google_result.get('answer')}\")\n",
    "print(f\"Sources: {google_result.get('sources')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8960b8b1",
   "metadata": {},
   "source": [
    "## 8. Ollama RAG Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db0d5ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Ollama RAG with model: gemma3n:latest\n",
      "Ollama RAG Result:\n",
      "Answer: The company's privacy note outlines your rights regarding your personal information, how to access or correct it, and how the company handles changes to its privacy policies. \n",
      "\n",
      "Here's a summary:\n",
      "\n",
      "*   **Right to Access and Correct:** You have the right to access, review, and request a copy of your personal information. You can also request that inaccurate information be corrected.\n",
      "*   **Contact Information:** To access or make changes to your information, contact the Privacy Officer at privacy@contoso.com.\n",
      "*   **Policy Changes:** The company may update its privacy policy and will notify you of changes by posting a revised policy on its website.\n",
      "*   **Questions or Concerns:** If you have questions or concerns about the privacy policies or practices, contact the Privacy Officer at privacy@contoso.com.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Sources: ['This document contains information generated using a language model (Azure OpenAI). The', 'This document contains information generated using a language model (Azure OpenAI). The', 'This document contains information generated using a language model (Azure OpenAI). The']\n"
     ]
    }
   ],
   "source": [
    "def ollama_rag_query(query: str, model_name: str = \"gemma3n:latest\") -> Dict[str, Any]:\n",
    "    \"\"\"Perform RAG query using Ollama.\"\"\"\n",
    "    # Retrieve relevant documents\n",
    "    relevant_docs = retrieve_relevant_docs(query)\n",
    "    \n",
    "    # Create prompt with context\n",
    "    prompt = create_rag_prompt(query, relevant_docs)\n",
    "    \n",
    "    # Generate response using chat API format\n",
    "    response = ollama.chat(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        options={\n",
    "            \"temperature\": 0.1\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"platform\": \"Ollama\",\n",
    "        \"model\": model_name,\n",
    "        \"answer\": response['message']['content'],\n",
    "        \"sources\": [doc['metadata']['title'] for doc in relevant_docs],\n",
    "        \"retrieved_docs\": relevant_docs\n",
    "    }\n",
    "\n",
    "# Test Ollama RAG\n",
    "test_query = \"What is the company's privacy note?\"\n",
    "print(f\"Testing Ollama RAG with model: gemma3n:latest\")\n",
    "ollama_result = ollama_rag_query(test_query)\n",
    "print(\"Ollama RAG Result:\")\n",
    "print(f\"Answer: {ollama_result.get('answer')}\")\n",
    "print(f\"Sources: {ollama_result.get('sources')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e132ff7e",
   "metadata": {},
   "source": [
    "## 9. Troubleshooting Guide\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31eca3a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Query: What is the company's roles ava\n",
      "\n",
      "⚠️ Before running this comparison, make sure you've fixed all model/deployment issues mentioned above\n",
      "Testing OpenAI...\n",
      "✅ OpenAI: Response generated\n",
      "   Sources: This document contains information generated using a language model (Azure OpenAI). The, This document contains information generated using a language model (Azure OpenAI). The, This document contains information generated using a language model (Azure OpenAI). The\n",
      "   Answer length: 1193 characters\n",
      "\n",
      "Testing Azure...\n",
      "✅ Azure: Response generated\n",
      "   Sources: This document contains information generated using a language model (Azure OpenAI). The, This document contains information generated using a language model (Azure OpenAI). The, This document contains information generated using a language model (Azure OpenAI). The\n",
      "   Answer length: 1190 characters\n",
      "\n",
      "Testing Google...\n",
      "✅ Google: Response generated\n",
      "   Sources: This document contains information generated using a language model (Azure OpenAI). The, This document contains information generated using a language model (Azure OpenAI). The, This document contains information generated using a language model (Azure OpenAI). The\n",
      "   Answer length: 1063 characters\n",
      "\n",
      "Testing Ollama...\n",
      "✅ Ollama: Response generated\n",
      "   Sources: This document contains information generated using a language model (Azure OpenAI). The, This document contains information generated using a language model (Azure OpenAI). The, This document contains information generated using a language model (Azure OpenAI). The\n",
      "   Answer length: 1022 characters\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def compare_rag_platforms(query: str) -> Dict[str, Any]:\n",
    "    \"\"\"Compare RAG implementation across all available platforms.\"\"\"\n",
    "    print(f\"🔍 Query: {query}\\n\")\n",
    "    print(\"⚠️ Before running this comparison, make sure you've fixed all model/deployment issues mentioned above\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Test each platform\n",
    "    platforms = [\n",
    "        (\"OpenAI\", openai_rag_query),\n",
    "        (\"Azure\", lambda q: azure_rag_query(q)),\n",
    "        (\"Google\", lambda q: google_rag_query(q)),\n",
    "        (\"Ollama\", lambda q: ollama_rag_query(q, model_name=\"gemma3n:latest\"))\n",
    "    ]\n",
    "    \n",
    "    for platform_name, rag_function in platforms:\n",
    "        print(f\"Testing {platform_name}...\")\n",
    "        try:\n",
    "            result = rag_function(query)\n",
    "            results[platform_name] = result\n",
    "            \n",
    "            if \"error\" in result:\n",
    "                print(f\"❌ {platform_name}: {result['error']}\")\n",
    "            else:\n",
    "                print(f\"✅ {platform_name}: Response generated\")\n",
    "                print(f\"   Sources: {', '.join(result.get('sources', []))}\")\n",
    "                print(f\"   Answer length: {len(result.get('answer', ''))} characters\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ {platform_name}: Error - {str(e)}\")\n",
    "            print(\"Check the platform-specific instructions above to resolve the issue\")\n",
    "            results[platform_name] = {\"error\": str(e)}\n",
    "        print()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run comparison test\n",
    "test_query = \"What is the company's roles ava\"\n",
    "comparison_results = compare_rag_platforms(test_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c628b58c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DETAILED COMPARISON RESULTS\n",
      "============================================================\n",
      "\n",
      "OpenAI Response:\n",
      "----------------------------------------\n",
      "Based on the provided context from the Employee Handbook, here is a list of the company roles available at Contoso Electronics:\n",
      "\n",
      "1. Chief Executive Officer\n",
      "2. Chief Operating Officer\n",
      "3. Chief Financial Officer\n",
      "4. Chief Technology Officer\n",
      "5. Vice President of Sales\n",
      "6. Vice President of Marketing\n",
      "7. Vice President of Operations\n",
      "8. Vice President of Human Resources\n",
      "9. Vice President of Research and Development\n",
      "10. Vice President of Product Management\n",
      "11. Director of Sales\n",
      "12. Director of Marketing\n",
      "13. Director of Operations\n",
      "14. Director of Human Resources\n",
      "15. Director of Research and Development\n",
      "16. Director of Product Management\n",
      "17. Senior Manager of Sales\n",
      "18. Senior Manager of Marketing\n",
      "19. Senior Manager of Operations\n",
      "20. Senior Manager of Human Resources\n",
      "21. Senior Manager of Research and Development\n",
      "22. Senior Manager of Product Management\n",
      "23. Manager of Sales\n",
      "24. Manager of Marketing\n",
      "25. Manager of Operations\n",
      "26. Manager of Human Resources\n",
      "27. Manager of Research and Development\n",
      "28. Manager of Product Management\n",
      "29. Sales Representative\n",
      "30. Customer Service Representative\n",
      "\n",
      "If you have any specific questions regarding these roles or need more information, feel free to ask.\n",
      "\n",
      "Sources used: This document contains information generated using a language model (Azure OpenAI). The, This document contains information generated using a language model (Azure OpenAI). The, This document contains information generated using a language model (Azure OpenAI). The\n",
      "\n",
      "Azure Response:\n",
      "----------------------------------------\n",
      "The company's job roles, as listed in the employee handbook, include the following:\n",
      "\n",
      "1. Chief Executive Officer  \n",
      "2. Chief Operating Officer  \n",
      "3. Chief Financial Officer  \n",
      "4. Chief Technology Officer  \n",
      "5. Vice President of Sales  \n",
      "6. Vice President of Marketing  \n",
      "7. Vice President of Operations  \n",
      "8. Vice President of Human Resources  \n",
      "9. Vice President of Research and Development  \n",
      "10. Vice President of Product Management  \n",
      "11. Director of Sales  \n",
      "12. Director of Marketing  \n",
      "13. Director of Operations  \n",
      "14. Director of Human Resources  \n",
      "15. Director of Research and Development  \n",
      "16. Director of Product Management  \n",
      "17. Senior Manager of Sales  \n",
      "18. Senior Manager of Marketing  \n",
      "19. Senior Manager of Operations  \n",
      "20. Senior Manager of Human Resources  \n",
      "21. Senior Manager of Research and Development  \n",
      "22. Senior Manager of Product Management  \n",
      "23. Manager of Sales  \n",
      "24. Manager of Marketing  \n",
      "25. Manager of Operations  \n",
      "26. Manager of Human Resources  \n",
      "27. Manager of Research and Development  \n",
      "28. Manager of Product Management  \n",
      "29. Sales Representative  \n",
      "30. Customer Service Representative  \n",
      "\n",
      "Let me know if you need further clarification or details about any specific role!\n",
      "\n",
      "Sources used: This document contains information generated using a language model (Azure OpenAI). The, This document contains information generated using a language model (Azure OpenAI). The, This document contains information generated using a language model (Azure OpenAI). The\n",
      "\n",
      "Google Response:\n",
      "----------------------------------------\n",
      "Based on the Contoso Electronics Employee Handbook, the following job roles are available:\n",
      "\n",
      "1.  Chief Executive Officer\n",
      "2.  Chief Operating Officer\n",
      "3.  Chief Financial Officer\n",
      "4.  Chief Technology Officer\n",
      "5.  Vice President of Sales\n",
      "6.  Vice President of Marketing\n",
      "7.  Vice President of Operations\n",
      "8.  Vice President of Human Resources\n",
      "9.  Vice President of Research and Development\n",
      "10. Vice President of Product Management\n",
      "11. Director of Sales\n",
      "12. Director of Marketing\n",
      "13. Director of Operations\n",
      "14. Director of Human Resources\n",
      "15. Director of Research and Development\n",
      "16. Director of Product Management\n",
      "17. Senior Manager of Sales\n",
      "18. Senior Manager of Marketing\n",
      "19. Senior Manager of Operations\n",
      "20. Senior Manager of Human Resources\n",
      "21. Senior Manager of Research and Development\n",
      "22. Senior Manager of Product Management\n",
      "23. Manager of Sales\n",
      "24. Manager of Marketing\n",
      "25. Manager of Operations\n",
      "26. Manager of Human Resources\n",
      "27. Manager of Research and Development\n",
      "28. Manager of Product Management\n",
      "29. Sales Representative\n",
      "30. Customer Service Representative\n",
      "\n",
      "Sources used: This document contains information generated using a language model (Azure OpenAI). The, This document contains information generated using a language model (Azure OpenAI). The, This document contains information generated using a language model (Azure OpenAI). The\n",
      "\n",
      "Ollama Response:\n",
      "----------------------------------------\n",
      "The company roles listed in the employee handbook are:\n",
      "\n",
      "1. Chief Executive Officer\n",
      "2. Chief Operating Officer\n",
      "3. Chief Financial Officer\n",
      "4. Chief Technology Officer\n",
      "5. Vice President of Sales\n",
      "6. Vice President of Marketing\n",
      "7. Vice President of Operations\n",
      "8. Vice President of Human Resources\n",
      "9. Vice President of Research and Development\n",
      "10. Vice President of Product Management\n",
      "11. Director of Sales\n",
      "12. Director of Marketing\n",
      "13. Director of Operations\n",
      "14. Director of Human Resources\n",
      "15. Director of Research and Development\n",
      "16. Director of Product Management\n",
      "17. Senior Manager of Sales\n",
      "18. Senior Manager of Marketing\n",
      "19. Senior Manager of Operations\n",
      "20. Senior Manager of Human Resources\n",
      "21. Senior Manager of Research and Development\n",
      "22. Senior Manager of Product Management\n",
      "23. Manager of Sales\n",
      "24. Manager of Marketing\n",
      "25. Manager of Operations\n",
      "26. Manager of Human Resources\n",
      "27. Manager of Research and Development\n",
      "28. Manager of Product Management\n",
      "29. Sales Representative\n",
      "30. Customer Service Representative\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Sources used: This document contains information generated using a language model (Azure OpenAI). The, This document contains information generated using a language model (Azure OpenAI). The, This document contains information generated using a language model (Azure OpenAI). The\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Display detailed comparison results\n",
    "print(\"DETAILED COMPARISON RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for platform, result in comparison_results.items():\n",
    "    if \"error\" not in result:\n",
    "        print(f\"\\n{platform} Response:\")\n",
    "        print(\"-\" * 40)\n",
    "        print(result.get('answer', 'No answer generated'))\n",
    "        print(f\"\\nSources used: {', '.join(result.get('sources', []))}\")\n",
    "    else:\n",
    "        print(f\"\\n{platform}: {result['error']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd551ce",
   "metadata": {},
   "source": [
    "## 10. Advanced RAG with Document Re-ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc5e8164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advanced RAG Result:\n",
      "==================================================\n",
      "Platform: OPENAI (Advanced RAG)\n",
      "Method: Re-ranked retrieval\n",
      "\n",
      "Answer:\n",
      "Unfortunately, the provided context does not include specific information about parental leave policies or benefits available to new parents at Contoso Electronics. To obtain accurate and detailed information regarding parental leave, I recommend consulting the company's employee handbook or contacting the Human Resources Department directly. They should be able to provide comprehensive details about the parental leave policy, including eligibility, duration, and any associated benefits. If you have access to the employee handbook, it would be beneficial to review the sections related to leave policies for more precise information.\n",
      "\n",
      "Sources with relevance scores:\n",
      "  - This document contains information generated using a language model (Azure OpenAI). The: 0.40\n",
      "  - This document contains information generated using a language model (Azure OpenAI). The: 0.33\n",
      "  - This document contains information generated using a language model (Azure OpenAI). The: 0.27\n"
     ]
    }
   ],
   "source": [
    "def advanced_rag_with_reranking(query: str, platform: str = \"openai\") -> Dict[str, Any]:\n",
    "    \"\"\"Implement RAG with document re-ranking for better results.\"\"\"\n",
    "    \n",
    "    # Get more documents initially\n",
    "    relevant_docs = retrieve_relevant_docs(query, n_results=6)\n",
    "    \n",
    "    # Simple re-ranking based on query similarity\n",
    "    query_words = set(query.lower().split())\n",
    "    \n",
    "    for doc in relevant_docs:\n",
    "        doc_words = set(doc['content'].lower().split())\n",
    "        word_overlap = len(query_words.intersection(doc_words))\n",
    "        doc['relevance_score'] = word_overlap / len(query_words) if query_words else 0\n",
    "    \n",
    "    # Sort by relevance and take top 3\n",
    "    relevant_docs.sort(key=lambda x: x['relevance_score'], reverse=True)\n",
    "    top_docs = relevant_docs[:3]\n",
    "    \n",
    "    # Create enhanced prompt\n",
    "    context = \"\\n\\n\".join([\n",
    "        f\"Source: {doc['metadata']['title']} (Relevance: {doc['relevance_score']:.2f})\\nContent: {doc['content']}\" \n",
    "        for doc in top_docs\n",
    "    ])\n",
    "    \n",
    "    prompt = f\"\"\"You are an expert AI assistant. Use the following ranked context to provide a comprehensive answer. Pay special attention to the most relevant sources.\n",
    "\n",
    "Ranked Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Provide a detailed answer with specific references to the sources:\"\"\"\n",
    "    \n",
    "    # Use OpenAI for this demo if available\n",
    "    if 'openai' in clients:\n",
    "        try:\n",
    "            response = clients['openai'].chat.completions.create(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are an expert HR assistant that provides detailed answers about company policies with source references from the employee handbook.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.1,\n",
    "                max_tokens=700\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                \"platform\": f\"{platform.upper()} (Advanced RAG)\",\n",
    "                \"answer\": response.choices[0].message.content,\n",
    "                \"sources\": [doc['metadata']['title'] for doc in top_docs],\n",
    "                \"relevance_scores\": [doc['relevance_score'] for doc in top_docs],\n",
    "                \"method\": \"Re-ranked retrieval\"\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"Advanced RAG failed: {str(e)}\"}\n",
    "    \n",
    "    return {\"error\": \"Platform not available for advanced RAG\"}\n",
    "\n",
    "# Test advanced RAG\n",
    "advanced_query = \"How does the company handle parental leave and what benefits are available to new parents?\"\n",
    "advanced_result = advanced_rag_with_reranking(advanced_query)\n",
    "\n",
    "print(\"Advanced RAG Result:\")\n",
    "print(\"=\" * 50)\n",
    "if \"error\" not in advanced_result:\n",
    "    print(f\"Platform: {advanced_result['platform']}\")\n",
    "    print(f\"Method: {advanced_result['method']}\")\n",
    "    print(f\"\\nAnswer:\\n{advanced_result['answer']}\")\n",
    "    print(f\"\\nSources with relevance scores:\")\n",
    "    for source, score in zip(advanced_result['sources'], advanced_result['relevance_scores']):\n",
    "        print(f\"  - {source}: {score:.2f}\")\n",
    "else:\n",
    "    print(f\"Error: {advanced_result['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec040f6",
   "metadata": {},
   "source": [
    "## 11. Performance Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4439d0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Benchmark Results\n",
      "==================================================\n",
      "\n",
      "Testing OpenAI...\n",
      "  Response time: 2.19s\n",
      "  Estimated tokens: 198\n",
      "  Answer quality: 1016 chars\n",
      "\n",
      "Testing Azure...\n",
      "  Response time: 1.76s\n",
      "  Estimated tokens: 76\n",
      "  Answer quality: 378 chars\n",
      "\n",
      "Testing Google...\n",
      "  Response time: 2.97s\n",
      "  Estimated tokens: 57\n",
      "  Answer quality: 250 chars\n",
      "\n",
      "Testing Ollama...\n",
      "  Response time: 14.93s\n",
      "  Estimated tokens: 119\n",
      "  Answer quality: 617 chars\n",
      "\n",
      "Platform     Time (s)   Tokens   Length   Sources \n",
      "--------------------------------------------------\n",
      "OpenAI       2.19       198      1016     3       \n",
      "Azure        1.76       76       378      3       \n",
      "Google       2.97       57       250      3       \n",
      "Ollama       14.93      119      617      3       \n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from typing import Tuple\n",
    "\n",
    "def measure_rag_performance(query: str, platform_function, platform_name: str) -> Tuple[Dict, float]:\n",
    "    \"\"\"Measure response time and token usage for RAG queries.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    result = platform_function(query)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    response_time = end_time - start_time\n",
    "    \n",
    "    # Estimate token usage (simplified)\n",
    "    if \"error\" not in result:\n",
    "        input_tokens = len(query.split()) * 1.3  # Rough estimate\n",
    "        output_tokens = len(result.get('answer', '').split()) * 1.3\n",
    "        total_tokens = input_tokens + output_tokens\n",
    "        \n",
    "        result['performance'] = {\n",
    "            'response_time': response_time,\n",
    "            'estimated_input_tokens': int(input_tokens),\n",
    "            'estimated_output_tokens': int(output_tokens),\n",
    "            'estimated_total_tokens': int(total_tokens)\n",
    "        }\n",
    "    \n",
    "    return result, response_time\n",
    "\n",
    "def performance_benchmark():\n",
    "    \"\"\"Run performance benchmark across platforms.\"\"\"\n",
    "    test_query = \"What are the company's policies on professional development and training?\"\n",
    "    \n",
    "    print(\"Performance Benchmark Results\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    platforms_to_test = []\n",
    "    if 'openai' in clients:\n",
    "        platforms_to_test.append((\"OpenAI\", openai_rag_query))\n",
    "    if 'azure' in clients:\n",
    "        platforms_to_test.append((\"Azure\", azure_rag_query))\n",
    "    if 'google' in clients:\n",
    "        platforms_to_test.append((\"Google\", google_rag_query))\n",
    "    if 'ollama' in clients:\n",
    "        platforms_to_test.append((\"Ollama\", ollama_rag_query))\n",
    "    \n",
    "    benchmark_results = []\n",
    "    \n",
    "    for platform_name, platform_function in platforms_to_test:\n",
    "        print(f\"\\nTesting {platform_name}...\")\n",
    "        \n",
    "        result, response_time = measure_rag_performance(test_query, platform_function, platform_name)\n",
    "        \n",
    "        if \"error\" not in result:\n",
    "            perf = result['performance']\n",
    "            print(f\"  Response time: {response_time:.2f}s\")\n",
    "            print(f\"  Estimated tokens: {perf['estimated_total_tokens']}\")\n",
    "            print(f\"  Answer quality: {len(result['answer'])} chars\")\n",
    "            \n",
    "            benchmark_results.append({\n",
    "                'platform': platform_name,\n",
    "                'response_time': response_time,\n",
    "                'tokens': perf['estimated_total_tokens'],\n",
    "                'answer_length': len(result['answer']),\n",
    "                'sources_used': len(result.get('sources', []))\n",
    "            })\n",
    "        else:\n",
    "            print(f\"  Error: {result['error']}\")\n",
    "    \n",
    "    # Summary\n",
    "    if benchmark_results:\n",
    "        print(f\"\\n{'Platform':<12} {'Time (s)':<10} {'Tokens':<8} {'Length':<8} {'Sources':<8}\")\n",
    "        print(\"-\" * 50)\n",
    "        for result in benchmark_results:\n",
    "            print(f\"{result['platform']:<12} {result['response_time']:<10.2f} {result['tokens']:<8} {result['answer_length']:<8} {result['sources_used']:<8}\")\n",
    "\n",
    "# Run benchmark\n",
    "performance_benchmark()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b8dd9a",
   "metadata": {},
   "source": [
    "## 12. Best Practices and Platform Comparison\n",
    "\n",
    "### Platform Comparison Summary:\n",
    "\n",
    "**OpenAI GPT Models:**\n",
    "- ✅ High-quality responses\n",
    "- ✅ Good API documentation\n",
    "- ✅ Reliable performance\n",
    "- ❌ Requires API key and costs money\n",
    "- ❌ Data sent to external service\n",
    "\n",
    "**Azure OpenAI:**\n",
    "- ✅ Enterprise-grade security\n",
    "- ✅ Integration with Azure ecosystem\n",
    "- ✅ Same model quality as OpenAI\n",
    "- ❌ More complex setup\n",
    "- ❌ Requires Azure subscription\n",
    "\n",
    "**Google AI Studio (Gemini):**\n",
    "- ✅ Competitive performance\n",
    "- ✅ Good integration with Google services\n",
    "- ✅ Multimodal capabilities\n",
    "- ❌ Newer platform, less mature\n",
    "- ❌ Limited model options\n",
    "\n",
    "**Ollama (Local):**\n",
    "- ✅ Complete privacy and control\n",
    "- ✅ No ongoing API costs\n",
    "- ✅ Works offline\n",
    "- ❌ Requires powerful hardware\n",
    "- ❌ Model quality may be lower\n",
    "- ❌ Slower response times\n",
    "\n",
    "### RAG Implementation Best Practices:\n",
    "\n",
    "1. **Document Chunking:** Keep chunks between 200-500 tokens\n",
    "2. **Embedding Models:** Use domain-specific models when available\n",
    "3. **Vector Store:** Choose based on scale (ChromaDB for small, Pinecone/Weaviate for production)\n",
    "4. **Retrieval:** Experiment with different similarity metrics\n",
    "5. **Prompt Engineering:** Include clear instructions about using context\n",
    "6. **Evaluation:** Implement metrics for answer quality and relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57932c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎉 RAG Implementation Complete!\n",
      "\n",
      "Next Steps:\n",
      "1. Configure your API keys in a .env file\n",
      "2. Install Ollama locally if you want to test local models\n",
      "3. Experiment with different embedding models\n",
      "4. Try your own documents and queries\n",
      "5. Implement evaluation metrics for your use case\n",
      "\n",
      "Available platforms in this session: ['openai', 'azure', 'google', 'ollama']\n",
      "\n",
      "Sample queries to try:\n",
      "1. What are the company's vacation and time off policies?\n",
      "2. How does the company handle performance reviews?\n",
      "3. What is the policy on remote work and flexible hours?\n",
      "4. Explain the company's benefits package and compensation structure\n",
      "\n",
      "Use the compare_rag_platforms() function to test any of these queries across all available platforms!\n"
     ]
    }
   ],
   "source": [
    "# Summary and next steps\n",
    "print(\"🎉 RAG Implementation Complete!\")\n",
    "print(\"\\nNext Steps:\")\n",
    "print(\"1. Configure your API keys in a .env file\")\n",
    "print(\"2. Install Ollama locally if you want to test local models\")\n",
    "print(\"3. Experiment with different embedding models\")\n",
    "print(\"4. Try your own documents and queries\")\n",
    "print(\"5. Implement evaluation metrics for your use case\")\n",
    "\n",
    "print(f\"\\nAvailable platforms in this session: {list(clients.keys())}\")\n",
    "\n",
    "# Display sample queries for testing\n",
    "sample_queries = [\n",
    "    \"What are the company's vacation and time off policies?\",\n",
    "    \"How does the company handle performance reviews?\",\n",
    "    \"What is the policy on remote work and flexible hours?\",\n",
    "    \"Explain the company's benefits package and compensation structure\"\n",
    "]\n",
    "\n",
    "print(\"\\nSample queries to try:\")\n",
    "for i, query in enumerate(sample_queries, 1):\n",
    "    print(f\"{i}. {query}\")\n",
    "    \n",
    "print(\"\\nUse the compare_rag_platforms() function to test any of these queries across all available platforms!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
